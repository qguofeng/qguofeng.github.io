---
layout: post
title:  "爬虫系列二"
crawlertitle: "爬虫爬取美女图片"
summary: "爬虫爬取美女图片"
date:   2017-01-29 23:09:47 +0700
categories: posts
tags: ['python','爬虫','全部文章']
author: QGUOFENG
---
![图片](/assets/active_images/img1.jpg)[附上爬取的图片中的两张]
本文章仅限于交流学习，如果侵犯爬取网站的利益，请联系我，本人将在第一时间删除该博文！！！

在平常学习中，尤其是前端的学习，总是需要很多的图片素材，美女图片的素材的需求也是很大的，但是从网上一个个的下载，很费劲，于是写了个爬虫来爬取素材公社中的美女图片素材!

<!--more-->
```bash
from bs4 import BeautifulSoup
import requests,urllib.request
import time


def get_one_page_img_link(url):
    imgs_link = []
    test=0
    """
    获取每页中的图片链接
    """
    #获取网页信息
    wb_data=requests.get(url)
    soup=BeautifulSoup(wb_data.text,'lxml')
    #获取图片地址
    img_a_link=soup.select('a.pic')
    #将图片存入列表imgs_link
    for img_link in img_a_link:
        imgs_link.append(img_link.get('href'))
    return imgs_link


def get_one_page_img(url):
    """
    获取图片所在链接的图片的真实地址
    """
    wb_data=requests.get(url)
    soup=BeautifulSoup(wb_data.text,'lxml')

    img=soup.select('#imgView')
    download_img(img[0].get('src'),filename)

def download_img(img, filename):
    """
    下载图片至filename
    """
    urllib.request.urlretrieve(img, filename+img[-16:])

def get_page_img(start,end):
    """
    你想下载几页的图片
    """
    for page in range(start,end+1):
        urls=url+str(page)+".aspx"
        for img in get_one_page_img_link(urls):
            get_one_page_img(img)


url='http://www.tooopen.com/img/88_282_1_'
#filename用来存放图片的路径
filename='/home/nborn/Documents/Python/note/python/Python_1_Network_Data_Collection/Other/one_weeks/3/img/img'
#用来存放图片所在网页链接
page1=input("请输入起始页数:")
page2=input("请输入结束页数:")
get_page_img(page1,page2)
```

![图片](/assets/active_images/img2.jpg)
