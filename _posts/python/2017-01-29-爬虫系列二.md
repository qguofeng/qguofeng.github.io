---
layout: post
title: "爬虫系列二"
date: 2017-01-29  13:00:00
description: "python爬虫"
tag: python爬虫
---
![图片](/assets/active_images/python/img1.jpg)



在平常学习中，尤其是前端的学习，总是需要很多的图片素材，美女图片的素材的需求也是很大的，但是从网上一个个的下载，很费劲，于是写了个爬虫来爬取素材公社中的美女图片素材!
### 代码

```bash
from bs4 import BeautifulSoup
import requests,urllib.request
import time


def get_one_page_img_link(url):
    imgs_link = []
    test=0
    """
    获取每页中的图片链接
    """
    #获取网页信息
    wb_data=requests.get(url)
    soup=BeautifulSoup(wb_data.text,'lxml')
    #获取图片地址
    img_a_link=soup.select('a.pic')
    #将图片存入列表imgs_link
    for img_link in img_a_link:
        imgs_link.append(img_link.get('href'))
    return imgs_link


def get_one_page_img(url):
    """
    获取图片所在链接的图片的真实地址
    """
    wb_data=requests.get(url)
    soup=BeautifulSoup(wb_data.text,'lxml')

    img=soup.select('#imgView')
    download_img(img[0].get('src'),filename)

def download_img(img, filename):
    """
    下载图片至filename
    """
    urllib.request.urlretrieve(img, filename+img[-16:])

def get_page_img(start,end):
    """
    你想下载几页的图片
    """
    for page in range(start,end+1):
        urls=url+str(page)+".aspx"
        for img in get_one_page_img_link(urls):
            get_one_page_img(img)


url='http://www.tooopen.com/img/88_282_1_'
#filename用来存放图片的路径
filename='/home/nborn/Documents/Python/note/python/Python_1_Network_Data_Collection/Other/one_weeks/3/img/img'
#用来存放图片所在网页链接
page1=input("请输入起始页数:")
page2=input("请输入结束页数:")
get_page_img(page1,page2)
```

![图片](/assets/active_images/python/img2.jpg)
[附上爬取的图片中的两张]
转载请注明：[邱国锋的博客](http://qiuguofeng.com) » [点击阅读原文](http://qiuguofeng.com/2017/01/爬虫系列二/)
